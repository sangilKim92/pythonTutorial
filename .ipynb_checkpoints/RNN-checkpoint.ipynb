{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "x= np.array([1,2,3])\n",
    "x.__class__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3,)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#담고있는 차원을 의미한다.\n",
    "x.ndim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 6  8 10]\n",
      "[ 5 12 21]\n",
      "[2 4 6]\n"
     ]
    }
   ],
   "source": [
    "x= np.array([1,2,3])\n",
    "y= np.array([5,6,7])\n",
    "print(x+y)\n",
    "print(x*y)\n",
    "print(x*2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 신경망은 가중치와 편향으로 구하는 최적의 함수, 기본적으로 완전연결계층으로 입력 데이터의 차원이 동일해야 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.28946303  0.87427204  0.09052527  0.99041114]\n",
      " [ 1.06870256  0.12014789 -1.10057388 -0.22900716]\n",
      " [ 1.8254601   0.55842887 -0.31450899 -1.70064367]\n",
      " [ 0.28055311 -0.70069875 -2.53459694  1.63992587]\n",
      " [-0.3092628   1.86173608  1.75689185  0.10711332]\n",
      " [ 1.41348588  0.35734388 -0.67908914 -0.93410976]\n",
      " [ 1.87406615 -0.21939203 -1.62509514 -1.05142525]\n",
      " [ 1.13681732  0.42178734 -0.58705038 -0.60341333]\n",
      " [-0.37024326  1.62465705  1.35282693  0.41188298]\n",
      " [-0.80425722  0.35464789 -0.8182236   2.1958881 ]]\n"
     ]
    }
   ],
   "source": [
    "W1 = np.random.randn(2,4)\n",
    "b1 = np.random.randn(4)\n",
    "x= np.random.randn(10,2)\n",
    "h = np.matmul(x,W1)+b1\n",
    "print(h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 위의 식은 간단하게 가중지 W1와 편향 b1을 이용한 입력 데이터 X의 결과값을 나타내는 것이다.\n",
    "- 위의 식의 입력층을 여러개 두면 신경망이 완성되지만 위의 식은 기본적으로 2차 행렬을 이용한 선형으로 완성된다.\n",
    "- 선형의 한계를 뛰어넘기 위해 활성화 함수로 비선형 함수를 사용한다.\n",
    "- 대표적인 활성화함수 ReLu, 시그모이드, 소프트맥스\n",
    "\n",
    "- 소프트맥스의 원소를 다 더하면 1이 되기에 확률로 사용할 수 있다.\n",
    "- 하지만 소프트맥스의 출력값을 원핫 인코딩으로 하나만 찾는다면 단순히 가장 큰 수를 찾으면 되기에 현업에서는 \n",
    "- 연산의 효율을 위해 큰 값을 찾는 것에 관해서는 소프트맥스를 사용안하고 최고값을 return하는 방법을 사용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.42813533 0.70563384 0.52261588 0.72916912]\n",
      " [0.7443501  0.53000089 0.24963238 0.44299712]\n",
      " [0.86122001 0.63608893 0.42201453 0.15438122]\n",
      " [0.56968182 0.33165732 0.07346812 0.83752485]\n",
      " [0.42329469 0.86549917 0.85281996 0.52675276]\n",
      " [0.80431518 0.58839731 0.33646463 0.28209168]\n",
      " [0.86692806 0.44537094 0.16450339 0.25895151]\n",
      " [0.75709482 0.60391087 0.35731192 0.35356317]\n",
      " [0.40848224 0.83543639 0.79459141 0.6015393 ]\n",
      " [0.3091156  0.58774422 0.30614087 0.89987965]]\n"
     ]
    }
   ],
   "source": [
    "print(sigmoid(h))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 위의 가중치와 편향으로 계산된 h에 대해 활성화함수를 적용한 값이다.\n",
    "- 그 다음 가중치를 다시 적용하고 활성화함수를 연속으로 적용해 가며 신경망을 구축한다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 손실함수는 교차 엔트로피, MSE가 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 손실함수의 미분을 줄이는 방식은 미분의 기울기로 구해도 되지만, 계산이 오래 걸리므로 오차역전파법을 사용한다.\n",
    "- 오차역전파법의 핵심은 아무리 함수가 쌓여 합성함수가 복잡해도 미분의 곱으로 계산할 수 있다는 점이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 역전파는 기본적으로 덧셈, 곱셈, 분기, sum, matmul 등의 식을 이용해 순전파가 진행되는 점을 이용해 \n",
    "- 덧셈은 미분값 1, 곱셈은 역, n분기는 미분*n을, sum은 기울기 /n을, matmul은 행렬의 역으로 식이 조금 복잡하다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MatMul:\n",
    "    def __init__(self, w):\n",
    "        self.params = [w]\n",
    "        self.grads = [np.zeros_like(w)]\n",
    "        self.x = None\n",
    "    \n",
    "    def forward(self, x):\n",
    "        w, = self.params\n",
    "        out = np.matmul(x,w)\n",
    "        self.x = x\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        w, = self.params\n",
    "        dx = np.matmul(dout, w.T)\n",
    "        dw = np.matmul(self.x.T, dout)\n",
    "        self.grads[0][...]  = dw\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- numpy의 데이터는 float64단위이다. 하지만, 신경망에서는 그렇게 비트가 클 필요가 없으므로 float32로 충분하고 메모리면에서 효율적이다.\n",
    "- 속도면에서도 32비트가 계산이 더 빠르다. CPU, GPU에 따라 다르긴 하다.\n",
    "- GPU가 계산에서 유리한 점, 딥러닝의 계산은 대부분 곱하기 연산으로 이루어지는데 병렬로 대량 곱을 할 수 있다는 점이 GPU가 효율적이다.\n",
    "\n",
    "- 파이썬의 GPU로 계산하는 cupy 라이브러리를 이용하면 된다.\n",
    "- numpy와 연동도 되고 사용법도 numpy와 같기에 좋다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 시소러스란 단어의 의미를 기반으로 유의어를 통해 컴퓨터가 글을 이해하게 하는 원리이다.\n",
    "- 모든 단어에 유의어 집합을 만든 다음, 단어들의 관계로 표현하고 정의한다.\n",
    "- 가장 유명한 시소러스는 WordNet\n",
    "\n",
    "\n",
    "- 시소러스의 문제점: 시대 변화에 대응하기 어렵다. \n",
    "- 모든 단어를 레이블하기에 사람을 쓰는 비용이 크다.\n",
    "- 단어의 미묘한 차이를 표현할 수 없다.\n",
    "\n",
    "- 이런 문제로 인해 통계 기반 기법과 추론 기반 기법이 나타났다.\n",
    "\n",
    "- 통계기반 기법은 말뭉치(맹목적으로 수집된 텍스트 데이터가 아닌 자연어 처리 연구를 염두에 두고 수집된 텍스트 데이터)를 이용한다.\n",
    "- 간단하게 통계기반 기법을 이용한 토큰화 및 레이블 잡억을 진행해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    text = text.lower() # 대문자를 소문자로 바꿔 동일하게 유지시킨다.\n",
    "    text = text.replace('.',' .') #점앞에 띄어쓰기를 추가한다. 토큰화 과정을 띄어쓰기로 한정하였고 예시문장이 갖는 특수문자가 . 밖에 없기에\n",
    "    words = text.split(' ')\n",
    "    word_to_id = {}\n",
    "    id_to_word= {}\n",
    "    for word in words:\n",
    "        if word not in word_to_id:\n",
    "            new_id = len(word_to_id)\n",
    "            word_to_id[word] = new_id\n",
    "            id_to_word[new_id]= word\n",
    "    \n",
    "    corpus = np.array([word_to_id[w] for w in words])\n",
    "    \n",
    "    return corpus, word_to_id, id_to_word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 이상으로 토큰화 과정을 끝마침 이제 통계 기반 기법을 활용하여 의미를 추출해보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"You say goodbye and I say Hello\"\n",
    "corpus,word_to_id, id_to_word = preprocess(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3 4 1 5]\n",
      "{'you': 0, 'say': 1, 'goodbye': 2, 'and': 3, 'i': 4, 'hello': 5}\n",
      "{0: 'you', 1: 'say', 2: 'goodbye', 3: 'and', 4: 'i', 5: 'hello'}\n"
     ]
    }
   ],
   "source": [
    "print(corpus)\n",
    "print(word_to_id)\n",
    "print(id_to_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 분산 표현이란 단어를 벡터화 시키는 것으로 컴퓨터가 쉽게 알 수 있게 단어를 벡터로 표현하는 것이다.\n",
    "- 분산 표현 방식의 기초는 '단어의 의미는 주변 단어에 의해 형성된다' 에 기초한다.\n",
    "- 이를 분포 가설이라 한다.\n",
    "- 통계 기반 기법은 어떤 단어가 나올 시 어떤 주변 단어가 몇 번 등장하는 지 세어 집계하는 방법이다.\n",
    "- 여기서 윈도우란 주변 단어의 거리 위치를 뜻한다. 1윈도우란 한 칸 떨어진 것까지 센다는 의미이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_co_matrix(corpus, vocab_size, window_size=2):\n",
    "    corpus_size = len(corpus)\n",
    "    co_matrix = np.zeros((vocab_size, vocab_size), dtype= np.int32)\n",
    "    \n",
    "    for idx, word_id in enumerate(corpus): #enumerate은 index와 값을 가져온다.\n",
    "        for i in range(1, window_size +1 ):\n",
    "            left_idx = idx -1 \n",
    "            right_idx = idx+1\n",
    "            \n",
    "            if left_idx>=0:\n",
    "                left_word_id = corpus[left_idx]\n",
    "                co_matrix[word_id, left_word_id] += 1\n",
    "                \n",
    "            if right_idx < corpus_size:\n",
    "                right_word_id = corpus[right_idx]\n",
    "                co_matrix[word_id, right_word_id] += 1\n",
    "                \n",
    "    return co_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cos_similarity(x,y, eps=1e-8):\n",
    "    nx = x / (np.sqrt(np.sum(x**2))+eps) #x의 정규화\n",
    "    ny = y / (np.sqrt(np.sum(y**2))+eps) #y의 정규화\n",
    "    return np.dot(nx,ny)\n",
    "\n",
    "#벡터의 유사도는 일반적으로 내적이나 유클리드 거리를 사용하지만, 단어 사이의 유사도는 코사인 유사도를 자주 사용한다.\n",
    "#코사인 유사도를 구현한 함수이다.\n",
    "#eps를 더한 이유는 만약 인수로 0 이 들어오면 나누기 오류가 발생하기에 아주 작은 값을 더해 주었다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_similar(query, word_to_id, id_to_word, word_matrix, top=5):\n",
    "    if query not in word_to_id:\n",
    "        print('%s를 찾을 수 없습니다' % query)\n",
    "        return\n",
    "    \n",
    "    print('\\n[query]'+query)\n",
    "    query_id = word_to_id[query]\n",
    "    query_vec = word_matrix[query_id]\n",
    "    \n",
    "    vocab_size = len(id_to_word)\n",
    "    similarity = np.zeros(vocab_size)\n",
    "    for i in range(vocab_size):\n",
    "        similarity[i] = cos_similarity(word_matrix[i], query_vec)\n",
    "        \n",
    "    count =0 \n",
    "    for i in(-1*similarity).argsort():\n",
    "        if id_to_word[i] == query:\n",
    "            continue\n",
    "        print(' %s: %s' %(id_to_word[i], similarity[i]))\n",
    "        \n",
    "        count += 1\n",
    "        if count>= top:\n",
    "            return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[query]you\n",
      " goodbye: 0.7071067691154799\n",
      " i: 0.7071067691154799\n",
      " hello: 0.7071067691154799\n",
      " say: 0.0\n",
      " and: 0.0\n"
     ]
    }
   ],
   "source": [
    "text= 'You say goodbye and I say hello.'\n",
    "corpus, word_to_id, id_to_word = preprocess(text)\n",
    "vocab_size = len(word_to_id)\n",
    "C = create_co_matrix(corpus, vocab_size)\n",
    "\n",
    "most_similar('you', word_to_id,id_to_word,C,top=5)\n",
    "#위의 통계기반 식을 적용해서 you와 가장 가깝다고 판단되는 단어를 코사인 유사도로 측정한 것이다.\n",
    "#goodbye가 높게 나온 이유는 자료의 부족 때문이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 고빈도만을 요인으로 벡터화 시키는 것은 상식적으로 생각해도 좋은 방식이 아니다.\n",
    "# the car인 경우 the 와 car은 상관관계가 크지 않지만 높은 빈도로 나타난다.\n",
    "# 이를 해결하기 위해서 점별 상호정보량이라는 척도를 사용한다.\n",
    "# 상호정보량은 단어의 빈도수에다가 단어가 독립적으로 나타나는 확률을 곱해서 계산한다.\n",
    "# ex) log(p(x,y)/p(x)p(y)) 이 식은 x,y가 동시에 나타날 확률을 x가 나타날 확률과 y가 나타날 확률의 곱으로 나눈 것이다.\n",
    "# 이 식도 값이 0으로 다가가면 log특성상 -무한대가 되기에 음수일 때는 0으로 취급한다.\n",
    "\n",
    "def ppmi(C, verbose=False, eps=1e-8):\n",
    "    M = np.zeros_like(C, dtype=np.float32)\n",
    "    N = np.sum(C)\n",
    "    S = np.sum(C, axis=0)\n",
    "    total = C.shape[0]*C.shape[1]\n",
    "    cnt =0\n",
    "    \n",
    "    for i in range(C.shape[0]):\n",
    "        for j in range(C>shape[1]):\n",
    "            pmi = np.log2(C[i,j]*N/(S[j]*S[i])+eps)\n",
    "            M[i,j]= max(0,pmi)\n",
    "            \n",
    "            if verbose:\n",
    "                cnt +=1\n",
    "                if cnt % (total//100) ==0:\n",
    "                    print('%.1f%% 완료' %(100*cnt/total))\n",
    "    return M\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 위의 식 통계 기반 방식은 문제점이 있는데\n",
    "# 대표적으로 말뭉치의 개수에 따라 차원의 수가 기하급수적으로 많아지고 대부분의 데이터가 0을 갖는다는 점입니다.\n",
    "#이를 해결하기 위해 차원 감소식을 적용하는데\n",
    "#차원 감소식은 numpy에 구현되어 있는 식 np.linalg.svd()로 대체해서 사용합니다.\n",
    "# 위의 식을 적용하면 데이터가 밀집된 형태의 차원으로 나타납니다.\n",
    "#하지만 위의 차원감소식도 계산량을 생각해보면 N= O(N^3)만큼 크다. 따라서 Truncated SVD를 사용한다.\n",
    "\n",
    "# 정리: 통계기반 기법은 말뭉치 안의 각 단어에 대해서 주변 단어에 의해 형성된다.(동시발생 행렬) PPMI행렬로 변환하고 다시 차원을 감소시킴으로써, 거대한\n",
    "#'희소벡터'를 작은'밀집벡터'로 변환한다.\n",
    "# 대부분 벡터화 기법들은 단어의 의미는 주변 단어에 의해 형성된다는 분포가설에 기초한다.\n",
    "#단어의 벡터 공간에서는 의미가 가까운 단어는 그 거리도 가까울 것으로 기대된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-4735613bfe25>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m#각 각의 단어에 인덱스 값을 부여하고 그 값에 대한 원핫 인코딩을 진행한다.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# you say goodbye and i hello . 의 you를 원핫 인코딩으로 표현한 것이다.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[0mW\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m7\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#신경망의 가중치를 나타낸 것이다.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mh\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mW\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#편향을 없애고 오직 인풋 데이터와 가중치만의 곱으로 표현한 값이다.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "#여기서부턴 추론 기반 기법 word2vec를 학습한다.\n",
    "#우선 신경망으로 처리하기 위해 문장을 고정길이 벡터로 변환한다. \n",
    "#문장을 계산하려면 벡터로 바꿔서 계산해야 하는데 원핫 인코딩이 가장 대표적이다.\n",
    "#각 각의 단어에 인덱스 값을 부여하고 그 값에 대한 원핫 인코딩을 진행한다.\n",
    "\n",
    "c = np.array([[1,0,0,0,0,0,0]]) # you say goodbye and i hello . 의 you를 원핫 인코딩으로 표현한 것이다.\n",
    "W = np.random.randn(7,3) #신경망의 가중치를 나타낸 것이다.\n",
    "h = np.matmul(c,W) #편향을 없애고 오직 인풋 데이터와 가중치만의 곱으로 표현한 값이다.\n",
    "print(h) #원핫 인코딩을 따르고(첫번째 값만 1 나머진 0) 거기에 곱한 것이기에\n",
    "# 사실 결과를 보자면 가중치의 가장 위쪽 행을 뽑아낸 것에 불과하다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3 4 1 5 6 7]\n"
     ]
    }
   ],
   "source": [
    "#찾고자 하는 데이터의 맥락을 원핫 인코딩으로 벡터화시키고 가중치와 곱한 것의 평균을 이용해서 출력을 구한다.\n",
    "#구한 출력값은 이후 softmax를 적용해 확률이 가장 높은 것이 추측값으로 설정하고 이를 교차 엔트로피로 손실값을 구한다.\n",
    "\n",
    "text = 'You say goodbye and I say hello .'\n",
    "corpus,word_to_id, id_to_word = preprocess(text)\n",
    "print(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#맥락같은 경우 가장 가까운 데이터 좌우하나씩을 포함하는 것으로 한다.\n",
    "def create_contexts_target(corpus, window_size=1):\n",
    "    target= corpus[window_size:-window_size]\n",
    "    contexts =[]\n",
    "    \n",
    "    for idx in range(window_size, len(corpus)-window_size):\n",
    "        cs=[]\n",
    "        for t in range(-window_size, window_size+1):\n",
    "            if t==0:\n",
    "                continue\n",
    "            cs.append(corpus[idx+t])\n",
    "        contexts.append(cs)\n",
    "    \n",
    "    return np.array(contexts), np.array(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "contexts, target = create_contexts_target(corpus,window_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 2]\n",
      " [1 3]\n",
      " [2 4]\n",
      " [3 1]\n",
      " [4 5]\n",
      " [1 6]\n",
      " [5 7]]\n",
      "[1 2 3 4 1 5 6]\n"
     ]
    }
   ],
   "source": [
    "print(contexts) #window_size를 1로 설정했기에 가까운 하나만 맥락으로 취급한다. 차례대로 1부터 6까지의 타겟과 맥락들이다.\n",
    "print(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#기본적으로 단어의 토큰화는 문장을 기반으로 순서대로 인덱스를 준 다음 모델링에 따라 다르게 신경망 계산을 진행한다.\n",
    "#이렇게 변환한 것들도 결국은 신경망 계산을 해야하기에 원핫 인코딩 시켜서 차원을 (6,2) 에서 (6,2,7) 로 바꾸고 \n",
    "#타켓의 차원도 (6,) 에서 (6,7)로 원핫으로 바꾼다.\n",
    "\n",
    "class SimpleCBOW:\n",
    "    def __init__(self, vocab_size, hidden_size):\n",
    "        V,H = vocab_size, hidden_size\n",
    "        \n",
    "        #가중치 초기화\n",
    "        W_in = 0.01*np.random.randn(V,H).astype('f')\n",
    "        W_out = 0.01*np.random.randn(V,H).astype('f')\n",
    "        \n",
    "        #계층 생성\n",
    "        #기본적으로 문맥의 개수만큼 matmul을 생성한다.\n",
    "        self.in_layer0= MatMul(W_in)\n",
    "        self.in_layer1=MatMul(W_in)\n",
    "        self.out_layer= MatMul(W_out)\n",
    "        self.loss_layer= SoftmaxWithLoss()\n",
    "        \n",
    "        layers = [self.in_layer0, slef.in_layer1, self.out_layer]\n",
    "        self.params, self.grads=[],[]\n",
    "        \n",
    "        for layer in layers:\n",
    "            self.params += layer.params\n",
    "            self.grads += layer.grads\n",
    "        \n",
    "        self.word_vecs = W_in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CBOW가 맥락을 통해 정답을 유추한다면\n",
    "# skip-gram은 정답을 통해 맥락을 유추하는 방식이다.\n",
    "#원핫 인코딩은 본질적인 문제가 있는데, 단어의 인덱스 값은 단어의 갯수가 많을수록 많아진다.\n",
    "#만약 단어가 10만개라면 인덱스값도 10만까지 증가한다. 이 값을 원핫 인코딩으로 계산하려면 10만개의 차원에 대해 연산을 해야 하는데\n",
    "#이는 불가능하다. \n",
    "#이를 해결하기 위해 word Embedding이라는 방식을 사용한다.의미를 최대한 담아 단어를 벡터로 바꾸는 단어 임베딩 모델\n",
    "\n",
    "# 통계기반 기법은 말뭉치의 전체 통계로부터 1회 학습하여 단어의 분산 표헌을 얻었다\n",
    "#추론 기반 기법에서는 말뭉치를 일부분씩 여러 번 보면서 학습했다.(미니배치 학습) \n",
    "#통계기반 기법은 단어가 추가될 때마다 새롭게 전체 데이터에 대해 새롭게 계산을 해야 한다.\n",
    "#추론 기반 기법은 여태까지 학습한 가중치를 초기 가중치로 설정해서 다시 학습하면 되어 추가 갱신이 간편해진다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding 계층이란\n",
    "\n",
    "- 사실 근본적으로 원핫 인코딩과 가중치의 곱은 특정한 행을 뽑아내는 것에 불과하다\n",
    "- 이를 이용하여 단어 ID에 해당하는 행(벡터)를 추출하는 계층을 만들고 그 계층을 Embedding 계층이라 표현한다\n",
    "- numpy의 특성을 이용하여 간단히 특정행을 뽑아내는 함수식\n",
    "- 단순히 가중치의 특정 행 뉴런만을 다음층으로 보내는 것이다.\n",
    "- 따라서 역전파에선 앞 층으로부터 전해진 기울기를 다음 층으로 그대로 보내면 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 지금 해결해야하는 문제\n",
    "\n",
    "- 현재 은닉층의 뉴런수가 토큰화 인덱스보다 작아야 성능이 좋아지는 이유를 모르겠음\n",
    "- embedding계층이란 것이 결국 행을 뽑아내서 행렬곱을 안하는 건데 과연 토큰화된 인덱스만큼 가중치는 가지는지 확인 필요\n",
    "- 토큰화된 인덱스가 2000개 인경우 가중치 (2000, 은닉층 뉴런수) 인지 아니면 padding해서 작게 한 것과 연관 되어있는지 확인 필요\n",
    "- 이때 은닉층 정보는 인간이 이해할 수 없는 코드로 쓰여 있다. (인코딩)\n",
    "- 한편, 은닉층의 정보로부터 원하는 결과를 얻는 작업은 디코딩이라고 한다.\n",
    "- 즉, 디코딩이란 인코딩된 정보를 인간이 이해할 수 있는 표현으로 복원하는 작업이다.\n",
    "- keras의 embedding도 단어의 인덱스 최대값을 가지고 그 외에도 사용자가 설정한 인풋 사이즈를 가진다.\n",
    "- 두 개를 다르게 설정하고 어떻게 계산하는지 확인 필요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedding:\n",
    "    def __init__(self, W):\n",
    "        self.params= [W]\n",
    "        self.grads =[np.zeros_like(W)]\n",
    "        self.idx= None\n",
    "        \n",
    "    def forward(self, idx):\n",
    "        W, = self.params\n",
    "        self.idx= idx\n",
    "        out = W[idx]\n",
    "        return out\n",
    "    \n",
    "    def backward(self,dout):\n",
    "        dW, = self.grads\n",
    "        dw[...]=0\n",
    "        dw[self.idx] = dout\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이렇게 입력 데이터와 가중치의 계산은 특성으로 줄였지만, 은닉층과 출력층의 곱 연산은 아직도 해결해야하는 문제이다.\n",
    "# 기존에는 어휘 수만큼의 뉴런을 준비하고 이 뉴런들이 출력한 값을 softmax 계층에 통과 시켰지만,\n",
    "# 이번에는 문맥값으로 뽑아낸 은닉층 뉴런 h와 출력 측의 가중치 W에서 정답 단어에 해당하는 단어 벡터와의 내적을 계산한다.\n",
    "# 그리고 그 출력을 sigmoid with loss 계층에 입력해 최종 손실을 얻는다.\n",
    "# 아래에 그 함수를 구현해보자\n",
    "\n",
    "class EmbeddingDot:\n",
    "    def __init__(self, W):\n",
    "        self.embed = Embedding(W)\n",
    "        self.params = self.embed.params\n",
    "        self.grads = self.embed.grads\n",
    "        self.cache = None\n",
    "        \n",
    "    def forward(self, h,idx):\n",
    "        target_W = self.embed.forward(idx)\n",
    "        out = np.sum(target_W*h, axis=1)\n",
    "        \n",
    "    def backward(self, dout):\n",
    "        h, target_W = self.cache\n",
    "        dout= dout.reshape(dout.shape[0],1)\n",
    "        \n",
    "        dtarget_W = dout*h\n",
    "        self.embed.backward(dtarget_W)\n",
    "        dh = dout * target_W\n",
    "        return dh\n",
    "\n",
    "#위의 식을 이용하여 우리는 정답값에 대해서 얼마나 맞는지 확률을 구하는 식을 구했다.\n",
    "#하지만 틀린 답인 경우 낮은 확률이 나타나도록 해야하는데 이때 네거티브 샘플링이 필요하다.\n",
    "#모든 어휘에 대해 연산시키면 안된다. 이를 해결하기 위해 생각하는 것이기 때문이다.\n",
    "\n",
    "#따라서 근사적인 해법을 부정적 예를 몇개 선택한다.\n",
    "#즉, 적은 수의 부정적 예를 샘플링해 사용한다.\n",
    "# 네거티브 샘플링의 샘플링 기법\n",
    "# 무작위로 하는 것보다 말뭉치의 통계 데이터를 기초로 한다.\n",
    "#간단히 말해 말뭉치에 자주 등장하는 것을 사용하고 드믈게 등장하는 단어를 적게 추출하는 방식이다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.random.choice(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 5, 1])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#리스트안에서 3개를 뽑고 중복을 제거해서 뽑는다.\n",
    "#확률 분포에 따라 뽑게 만들 수 있는데 이를 이용하여 네거티브 샘플링의 샘플링을 얻는다.\n",
    "#p라는 인수에 확률을 넣어주면 확률분포로 판단해 그 확률분포대로 값을 가져온다.\n",
    "np.random.choice([1,2,3,4,5],size=3, replace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NegativeSamplingLoss:\n",
    "    def __init__(self, W, corpus, power = 0.75, sample_size=5):\n",
    "        self.sample_size = sample_size\n",
    "        self.sampler = UnigramSampler(corpus, power, sample_size)\n",
    "        self.loss_layers = [SigmoidWithLoss() for _ in range(sample_size +1)]\n",
    "        self.embed_dot_layers = [EmbeddingDot(W) for _ in range(sample_size+1)]\n",
    "        \n",
    "        self.params , self.grads = [], []\n",
    "        for layer in self.embed_dot_layers:\n",
    "            self.params += layer.params\n",
    "            self.grads += layer.grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
