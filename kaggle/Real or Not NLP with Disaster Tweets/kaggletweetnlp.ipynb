{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom numpy import array\nfrom nltk import word_tokenize\nfrom sklearn.model_selection import train_test_split\nimport re\nimport nltk\nfrom nltk.stem import WordNetLemmatizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import ensemble\n\n\n#주어진 문장을 '단어'로 토큰화 하기\nimport gensim.models\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n#케라스의 텍스트 전처리와 관련한 함수중 text_to_word_sequence 함수를 불러 옵니다.\nfrom tensorflow.keras.preprocessing.text import text_to_word_sequence\n ","execution_count":7,"outputs":[{"output_type":"stream","text":"/kaggle/input/nlp-getting-started/train.csv\n/kaggle/input/nlp-getting-started/test.csv\n/kaggle/input/nlp-getting-started/sample_submission.csv\n","name":"stdout"}]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train = pd.read_csv('/kaggle/input/nlp-getting-started/train.csv')\ntest = pd.read_csv('/kaggle/input/nlp-getting-started/test.csv')","execution_count":8,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train","execution_count":9,"outputs":[{"output_type":"execute_result","execution_count":9,"data":{"text/plain":"         id keyword location  \\\n0         1     NaN      NaN   \n1         4     NaN      NaN   \n2         5     NaN      NaN   \n3         6     NaN      NaN   \n4         7     NaN      NaN   \n...     ...     ...      ...   \n7608  10869     NaN      NaN   \n7609  10870     NaN      NaN   \n7610  10871     NaN      NaN   \n7611  10872     NaN      NaN   \n7612  10873     NaN      NaN   \n\n                                                   text  target  \n0     Our Deeds are the Reason of this #earthquake M...       1  \n1                Forest fire near La Ronge Sask. Canada       1  \n2     All residents asked to 'shelter in place' are ...       1  \n3     13,000 people receive #wildfires evacuation or...       1  \n4     Just got sent this photo from Ruby #Alaska as ...       1  \n...                                                 ...     ...  \n7608  Two giant cranes holding a bridge collapse int...       1  \n7609  @aria_ahrary @TheTawniest The out of control w...       1  \n7610  M1.94 [01:04 UTC]?5km S of Volcano Hawaii. htt...       1  \n7611  Police investigating after an e-bike collided ...       1  \n7612  The Latest: More Homes Razed by Northern Calif...       1  \n\n[7613 rows x 5 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>keyword</th>\n      <th>location</th>\n      <th>text</th>\n      <th>target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Our Deeds are the Reason of this #earthquake M...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>4</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Forest fire near La Ronge Sask. Canada</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>5</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>All residents asked to 'shelter in place' are ...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>6</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>13,000 people receive #wildfires evacuation or...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>7</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>7608</th>\n      <td>10869</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Two giant cranes holding a bridge collapse int...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>7609</th>\n      <td>10870</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>@aria_ahrary @TheTawniest The out of control w...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>7610</th>\n      <td>10871</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>M1.94 [01:04 UTC]?5km S of Volcano Hawaii. htt...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>7611</th>\n      <td>10872</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Police investigating after an e-bike collided ...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>7612</th>\n      <td>10873</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>The Latest: More Homes Razed by Northern Calif...</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n<p>7613 rows × 5 columns</p>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['keyword'].value_counts()","execution_count":26,"outputs":[{"output_type":"execute_result","execution_count":26,"data":{"text/plain":"fatalities               45\ndeluge                   42\narmageddon               42\nharm                     41\nbody%20bags              41\n                         ..\nforest%20fire            19\nepicentre                12\nthreat                   11\ninundation               10\nradiation%20emergency     9\nName: keyword, Length: 221, dtype: int64"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train.groupby('keyword')['target'].mean().sort_values())","execution_count":31,"outputs":[{"output_type":"stream","text":"keyword\naftershock     0.000000\nbody%20bags    0.024390\nruin           0.027027\nblazing        0.029412\nbody%20bag     0.030303\n                 ...   \ntyphoon        0.973684\noutbreak       0.975000\ndebris         1.000000\nwreckage       1.000000\nderailment     1.000000\nName: target, Length: 221, dtype: float64\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train.groupby('keyword')['target'].mean())","execution_count":34,"outputs":[{"output_type":"stream","text":"keyword\nablaze                 0.361111\naccident               0.685714\naftershock             0.000000\nairplane%20accident    0.857143\nambulance              0.526316\n                         ...   \nwounded                0.702703\nwounds                 0.303030\nwreck                  0.189189\nwreckage               1.000000\nwrecked                0.076923\nName: target, Length: 221, dtype: float64\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('train[\\'text\\']의 결측치 개수', train['text'].isnull().sum())\nprint('test[\\'text\\']의 결측치 개수', test['text'].isnull().sum())\nprint('train[\\'keyword\\']의 결측치 개수', train['keyword'].isnull().sum())\nprint('test[\\'keyword\\']의 결측치 개수', test['keyword'].isnull().sum())\nprint('train[\\'location\\']의 결측치 개수', train['location'].isnull().sum())\nprint('test[\\'location\\']의 결측치 개수', test['location'].isnull().sum())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corpus=train['text'].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corpus=np.concatenate((corpus,test['text'].values), axis=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corpus.size","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"temp=[]\nfor tem in corpus:\n    temp.append(re.sub(r'[^a-zA-Z0-9#@ ]',' ',tem).lower())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corpus=np.array(temp)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corpus","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lm = WordNetLemmatizer()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"temp=[]\nfor tem in corpus:\n    temp.append(tem.split())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"temp2=[]\nfor temp_ in temp:\n    temp2.append([lm.lemmatize(w, pos=\"v\") for w in temp_])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corpus=[]\nfor tem2 in temp2:\n    corpus.append(' '.join(tem2))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corpus","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Word2Vec 생성\nsentences = []\n#reviews = clean_train_reviews\nfor review in corpus:\n    sentences.append(review.split())   # [['stuff',  'going',  'moment',  'mj',  'started',  'listening',  'music', ... ], ...] #25000개의 리스트\n\nprint(len(sentences))  #25000\n\nfrom gensim.models import word2vec\nword2vec_model = word2vec.Word2Vec(sentences, workers=1, size=10, min_count = 2, window = 10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"temp=train['keyword'].values\naa=[]\nfor tem in temp:\n    if tem is not np.nan:\n        aa.append(re.sub(r'[^a-zA-Z]',' ',tem).lower().split())\n#    else:\n #       aa.append('')\ntemp=[]\nfor a in aa:\n    temp.append([lm.lemmatize(w,pos='v') for w in a])\n\nmean = np.zeros((10), dtype=np.float32) # keyword의 평균값구하는 식\ntime = 0\nfor a in temp:\n    time = time +1\n    for b in a:\n        mean = mean + word2vec_model[b]\n        time = time +1\n    \nmean = mean / time ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mean #keyword의 평균값","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"temp=train['keyword'].values\naa=[]\nfor tem in temp:\n    if tem is not np.nan:\n        aa.append(re.sub(r'[^a-zA-Z]',' ',tem).lower().split())\n    else:\n        aa.append('')\ntemp=[]\nfor a in aa:\n    temp.append([lm.lemmatize(w,pos='v') for w in a])\n\nvec = np.empty(0) # keyword의 평균값구하는 식 각 키워드에 대해 벡터화 시키는 코드\nprint(vec)\ntime = 0\nfor a in temp:\n    time = 0\n    numpy = np.zeros((10), dtype=np.float32)\n    if not a:\n        time=1\n        numpy = numpy + mean\n    else:    \n        for b in a:\n            time=time+1\n            if b in word2vec_model:\n                numpy = numpy + word2vec_model[b] # 값이 있으면 그냥 더하기\n            else:\n                numpy = numpy + mean #평균값 더하기\n        numpy = numpy / time # 여러 단어의 keyword로 구성되었다면 평균으로 나누어 주기\n    vec=np.concatenate((vec,numpy), axis=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vec=vec.reshape(-1,10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vec.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#텍스트만을 독립변수로 했을 때의 적중률이 상당히 낮게 나왔음\n#텍스트의 성질을 분석하여 int형으로 할당한 후 신경망 계산\n#분산표현을 사용하는 곳은 keyword 칼럼\ntrain['ht_number'] = train['text'].apply(lambda x: len([c for c in str(x) if c == '#']))\ntest['ht_number'] = test['text'].apply(lambda x: len([c for c in str(x) if c == '#']))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ntrain['email_number'] = train['text'].apply(lambda x: len([c for c in str(x) if c == '@']))\ntest['email_number'] = test['text'].apply(lambda x: len([c for c in str(x) if c == '@']))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['text_length'] = train['text'].apply(lambda x: len(x))\ntest['text_length'] = test['text'].apply(lambda x: len(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = train.drop('id',axis=1)\ntrain = train.drop('location',axis=1)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train['ht_number'].values.size)\nprint(train['email_number'].values.size)\nprint(train['text_length'].values.size)\nprint(vec.shape)\nprint(vec[0])\nvec= np.append(vec,train['ht_number'].values.reshape(-1,1),axis=1)\nvec= np.append(vec,train['email_number'].values.reshape(-1,1),axis=1)\nvec= np.append(vec,train['text_length'].values.reshape(-1,1),axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vec.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#vec를 keyword의 벡터화시키고, email 개수, 해쉬태그 개수, 문장의 길이 데이터를 numpy 배열에 추가","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = train.values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Y_train = train[:,2]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = vec","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Y_train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test= train_test_split(X_train, Y_train, test_size=0.3, random_state=23)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(X_train.shape)\nprint(y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nwineRFModel = ensemble.RandomForestRegressor(n_estimators=100,\n                    max_depth=50,oob_score=False, random_state=31)\nwineRFModel.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"prediction = wineRFModel.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"collect=np.sum(np.around(prediction)==y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"collect/len(y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"temp=test['keyword'].values\naa=[]\nfor tem in temp:\n    if tem is not np.nan:\n        aa.append(re.sub(r'[^a-zA-Z]',' ',tem).lower().split())\n    else:\n        aa.append('')\ntemp=[]\nfor a in aa:\n    temp.append([lm.lemmatize(w,pos='v') for w in a])\n\nvec = np.empty(0) # keyword의 평균값구하는 식 각 키워드에 대해 벡터화 시키는 코드\nprint(vec)\ntime = 0\nfor a in temp:\n    time = 0\n    numpy = np.zeros((10), dtype=np.float32)\n    if not a:\n        time=1\n        numpy = numpy + mean\n    else:    \n        for b in a:\n            time=time+1\n            if b in word2vec_model:\n                numpy = numpy + word2vec_model[b] # 값이 있으면 그냥 더하기\n            else:\n                numpy = numpy + mean #평균값 더하기\n        numpy = numpy / time # 여러 단어의 keyword로 구성되었다면 평균으로 나누어 주기\n    vec=np.concatenate((vec,numpy), axis=0)\nvec=vec.reshape(-1,10)\n\nvec= np.append(vec,test['ht_number'].values.reshape(-1,1),axis=1)\nvec= np.append(vec,test['email_number'].values.reshape(-1,1),axis=1)\nvec= np.append(vec,test['text_length'].values.reshape(-1,1),axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"answer=np.round(wineRFModel.predict(vec)).astype(int)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.read_csv('/kaggle/input/nlp-getting-started/sample_submission.csv')\nsubmission['target'] = answer","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nsubmission.to_csv('submission.csv', index=False, header=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"answer","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}